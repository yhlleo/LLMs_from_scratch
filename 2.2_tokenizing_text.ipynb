{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c52fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x7fc0a05ddc70>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e2d7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_data))\n",
    "print(raw_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf77377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9dff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r\"([,.]|\\s)\", text) # splits on whitespace, commas, and periods\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0238c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [it for it in result if it.strip()] # remove whitespace charachters\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1d85ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [it.strip() for it in result if it.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32ae220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    result = [it.strip() for it in result if it.strip()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a1d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = tokenizer(raw_data) # preprocess the whole text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f0bad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690 ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed), preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2d8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) # sorting the unique tokens\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e04aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n",
      "Burlington 21\n",
      "But 22\n",
      "By 23\n",
      "Carlo 24\n",
      "Chicago 25\n",
      "Claude 26\n",
      "Come 27\n",
      "Croft 28\n",
      "Destroyed 29\n",
      "Devonshire 30\n",
      "Don 31\n",
      "Dubarry 32\n",
      "Emperors 33\n",
      "Florence 34\n",
      "For 35\n",
      "Gallery 36\n",
      "Gideon 37\n",
      "Gisburn 38\n",
      "Gisburns 39\n",
      "Grafton 40\n",
      "Greek 41\n",
      "Grindle 42\n",
      "Grindles 43\n",
      "HAD 44\n",
      "Had 45\n",
      "Hang 46\n",
      "Has 47\n",
      "He 48\n",
      "Her 49\n",
      "Hermia 50\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:index for index,token in enumerate(all_words)}\n",
    "for i, item in vocab.items():\n",
    "    print(i, item)\n",
    "    if item >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e494d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.str2int = vocab\n",
    "        self.int2str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text): # string to token ids\n",
    "        preprocessed = tokenizer(text)\n",
    "        ids = [self.str2int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids): # token ids to string\n",
    "        text = \" \".join([self.int2str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # remove spaces before the specified punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26744966",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1cc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know, \"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = my_tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b41b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3ff82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) # sorting the unique tokens\n",
    "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c493a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:index for index,token in enumerate(all_words)}\n",
    "for i, item in list(enumerate(vocab.items()))[-5:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48ec21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.str2int = vocab\n",
    "        self.int2str = {i:s for s,i in vocab.items()}\n",
    "        self.unk = \"<|unk|>\"\n",
    "        self.eof = \"<|endoftext|>\"\n",
    "        \n",
    "    def encode(self, text): # string to token ids\n",
    "        preprocessed = tokenizer(text)\n",
    "        ids = [self.str2int[s] if s in self.str2int else self.str2int[self.unk] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids): # token ids to string\n",
    "        text = \" \".join([self.int2str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # remove spaces before the specified punctuation\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8f4ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45efcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b6d7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n"
     ]
    }
   ],
   "source": [
    "ids = my_tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6017a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bfff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
