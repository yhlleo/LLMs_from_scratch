# LLMs_from_scratch
Learning records for building a large language model from scratch

### Records

 - **Section 2**:

- [x] Understanding word embeddings

- [x] Tokenizing text

- [x] Converting tokens into token IDs

- [x] Adding special context tokens

- [x] Byte pair encoding (TODO: more details)

- [x] Data sampling with a sliding window

- [x] Creating token embeddings

- [x] Encoding word positions

 - **Section 3**:

- [x] Capturing data dependencies with attention mechanisms

- [x] Implementing self-attention with trainable weights

- [x] Hiding feature words with causal attention

- [x] Multi-head attention

 - **Section 4**:

- [x] Activations and layer normalization

- [x] Adding shortcut connections

- [x] Build a Transformer block

- [x] Coding the GPT model

- [x] Generating text

 - **Section 5**:

- [x] Evaluating generative text model

- [x] Training an LLM

![](ch05/train_plot.png)

- [x] Greedy search and Top-k sampling

- [x] Load pretrained weights from OpenAI

 - **Section 6**:

- [x] Prepare spam email dataset and dataloader

- [x] Fine-tune the model on supervised data

- [x] Use the LLM as a spam classifier

 - **Section 7:**

- [x] Prepare a dataset for supervised instruction fine-tuning

- [x] Organize data into training batches

- [ ] Finetune the LLM on instruction data



